{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Churn Model Scoring\n",
    "\n",
    "### Step 1: Download new customer data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "url_customer='https://raw.githubusercontent.com/DScienceAtScale/DSX/master/PredictCustomerChurn/Data/new_customer_churn_data.csv'\n",
    "\n",
    "#remove existing files before downloading\n",
    "!rm -f new_customer_churn_data.csv\n",
    "\n",
    "customerFilename=wget.download(url_customer)\n",
    "\n",
    "!ls -l new_customer_churn_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read data into a DataFrame\n",
    "Note: the new dataset does not contain the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newData= sqlContext.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(customerFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = newData.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\n",
    "newData.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load Saved Model\n",
    "Load model in Object Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "model1_loaded = PipelineModel.load(\"PredictChurn.churnModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Score the new data\n",
    "Note: The scored output contains the predicted values and confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model1_loaded.transform(newData)\n",
    "results.toPandas().head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Export Score into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select ID, prediction and probability fields from the results dataframe\n",
    "\n",
    "r1=results.select(results[\"ID\"],results[\"prediction\"],results[\"probability\"])\n",
    "r1.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decompose the probability column\n",
    "The probability column contains a vector for each record, and the elements must be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "udf_0 = udf(lambda vector: float(vector[0]), DoubleType())\n",
    "udf_1 = udf(lambda vector: float(vector[1]), DoubleType())\n",
    "\n",
    "r2 = (r1.select(r1[\"ID\"], r1[\"prediction\"],r1[\"probability\"])\n",
    "    .withColumn('probability_0', udf_0(r1.probability))\n",
    "    .withColumn('probability_1', udf_1(r1.probability))\n",
    "    .drop(\"probability\"))\n",
    "\n",
    "r2.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write scores to csv file in local storage. (save it in a local subdirectory called 'PredictChurn/' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spark 2.0 method to write should use the new csv(path) class which is equivalent to format(\"csv\").save('path'). However due to current glitch in Spark environment where two csv \n",
    "# packages coexist, this will return an error about duplicate sources for the csv class.\n",
    "# r2.write.csv('SparkdayMelbourne.' + 'churn_scores.csv', mode='overwrite')\n",
    "\n",
    "# instead of format('csv'), we provide the fully qualified name due to problem mentioned above (currently two csv packages causing resolution issues)\n",
    "r2.write.format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat').save('PredictChurn/' + 'churn_scores.csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls PredictChurn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that the csv file can be read back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same issue as above. read.csv('path') will currently fail, so resorting to the older approach with fully qualified csv class\n",
    "#r3= spark.read.csv('SparkdayMelbourne.' + 'churn_scores.csv')\n",
    "r3= spark.read.format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat').load('PredictChurn/' + 'churn_scores.csv')\n",
    "\n",
    "r3.select(r3[\"_c0\"].alias(\"ID\"), r3[\"_c1\"].alias(\"prediction\"), r3[\"_c2\"].alias(\"probability_0\"), r3[\"_c3\"].alias(\"probability_1\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf PredictChurn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write scores to csv file but to object store this time (more involved, as requires credentials, so <span style=\"color:red\"> individual customization </span> needed for each user running this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Object Storage\n",
    "In order to write the scores to Object Storage, specify the credentials to connect to your instance of Object Storage.  The easiet way to do that is:\n",
    "- If you do not already have a file in Object Storage, load a file into it using the **Files** interface\n",
    "- Click on the provided blank cell right below this one.\n",
    "- Choose \"*Insert SparkSession DataFame*\" to generate the credentials and code to connect to Object Storage\n",
    "\n",
    "![Load Files](https://raw.githubusercontent.com/DScienceAtScale/DSX/master/PredictCustomerChurn/Images/upload_files.png)\n",
    "\n",
    "- Edit the code to comment out or edit the code that reads the file.  The edited code cell should look like this\n",
    "\n",
    "![credentials](https://raw.githubusercontent.com/DScienceAtScale/DSX/master/PredictCustomerChurn/Images/generated_credentials.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once you have at least one file available in the object storage container associated with this project, select the blank cell below this one (click on it), then click on the icon highlighted in the picture above (icon with a pattern of '1001') and then choose \"Insert SparkSession DataFrame\". \n",
    "## Some code similar to what is displayed in the picture above should be automatically inserted in the blank cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure you replace the <span style=\"color:blue\">XXXXXX</span> string below with the <span style=\"color:blue\">name of the container</span> for your current project (by default the same name as the project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ingest.Connectors import Connectors\n",
    "\n",
    "objectstoresaveOptions = {\n",
    "        Connectors.BluemixObjectStorage.AUTH_URL          : credentials['auth_url'],\n",
    "        Connectors.BluemixObjectStorage.USERID            : credentials['user_id'],\n",
    "        Connectors.BluemixObjectStorage.PASSWORD          : credentials['password'],\n",
    "        Connectors.BluemixObjectStorage.PROJECTID         : credentials['project_id'],\n",
    "        Connectors.BluemixObjectStorage.REGION            : credentials['region'],\n",
    "        Connectors.BluemixObjectStorage.TARGET_CONTAINER  : 'XXXXXX',\n",
    "        Connectors.BluemixObjectStorage.TARGET_FILE_NAME  : 'churn_scores.csv',\n",
    "        Connectors.BluemixObjectStorage.TARGET_WRITE_MODE : 'write'}\n",
    "\n",
    "\n",
    "r2.write.format(\"com.ibm.spark.discover\").options(**objectstoresaveOptions).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure you replace the <span style=\"color:blue\">XXXXXX</span> string below with the <span style=\"color:blue\">name of the container</span> for your current project (by default the same name as the project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r3 = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .load(bmos.url('XXXXXX', 'churn_scores.csv'))\n",
    "r3.select(r3[\"_c0\"].alias(\"ID\"), r3[\"_c1\"].alias(\"prediction\"), r3[\"_c2\"].alias(\"probability_0\"), r3[\"_c3\"].alias(\"probability_1\")).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 6: Schedule this notebook to run at a time and frequency of your choice\n",
    "Click on the \"clock\" icon at the top right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have come to the end of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Sidney Phoon** <br/>\n",
    "yfphoon@us.ibm.com<br/>\n",
    "May 4th, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
